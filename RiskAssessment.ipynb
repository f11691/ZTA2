{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b5270-10b2-44b8-9a2c-eb825fc1ee75",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# risk_assessment.py\n",
    "\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np  # Make sure to install numpy if not already installed\n",
    "import logging\n",
    "import psutil\n",
    "import os\n",
    "import uuid\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class RiskAssessment:\n",
    "    def __init__(self, pe_url=None):\n",
    "        # Stores risk level, no-anomaly rounds, history, and past anomalies for each entity\n",
    "        self.entity_risks = {}  # {entity_id: {...}}\n",
    "        # Mapping of attack types to criticality values (A)\n",
    "        self.attack_criticality = {\n",
    "            'privilege_escalation': 9,\n",
    "            'phishing': 7,\n",
    "            'lateral_movement': 8,\n",
    "            'data_exfiltration': 10,\n",
    "            # Add more attack types as needed\n",
    "        }\n",
    "        # Mapping of entities to segments\n",
    "        self.entity_segments = {\n",
    "            'iot_device1': 'production_network',\n",
    "            'iot_device2': 'production_network',\n",
    "            'user': 'user_network',\n",
    "            'server': 'data_center',\n",
    "            # Add more entities as needed\n",
    "        }\n",
    "        # Mapping of segments to criticality values (S)\n",
    "        self.segment_criticality = {\n",
    "            'production_network': 10,\n",
    "            'data_center': 10,\n",
    "            'user_network': 5,\n",
    "            'default_segment': 5,\n",
    "            # Add more segments as needed\n",
    "        }\n",
    "        self.pe_url = pe_url  # URL of the Policy Engine\n",
    "        self.load_data()  # Load saved data\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        # Configure the logging module\n",
    "        logging.basicConfig(\n",
    "            filename='risk_assessment.log',\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s %(levelname)s: %(message)s'\n",
    "        )\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            with open('entity_risks.json', 'r') as f:\n",
    "                self.entity_risks = json.load(f)\n",
    "            print(\"Data loaded from entity_risks.json\")\n",
    "        except FileNotFoundError:\n",
    "            # File doesn't exist yet\n",
    "            self.entity_risks = {}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            self.entity_risks = {}\n",
    "\n",
    "    def save_data(self):\n",
    "        try:\n",
    "            with open('entity_risks.json', 'w') as f:\n",
    "                json.dump(self.entity_risks, f)\n",
    "            print(\"Data saved to entity_risks.json\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data: {e}\")\n",
    "\n",
    "    def record_anomaly(self, entity_id, attack_type, confidence, request_id):\n",
    "        # Start CPU and timing measurements\n",
    "        process = psutil.Process(os.getpid())\n",
    "        cpu_times_start = process.cpu_times()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Initialize entity data if not present\n",
    "        if entity_id not in self.entity_risks:\n",
    "            self.entity_risks[entity_id] = {\n",
    "                'risk_level': 'normal',\n",
    "                'no_anomaly_rounds': 0,\n",
    "                'history': [],\n",
    "                'past_anomalies_count': 0\n",
    "            }\n",
    "        else:\n",
    "            self.entity_risks[entity_id]['no_anomaly_rounds'] = 0\n",
    "\n",
    "        # Increment past anomalies count\n",
    "        self.entity_risks[entity_id]['past_anomalies_count'] += 1\n",
    "\n",
    "        # Confidence of Threat (C)\n",
    "        C = confidence  # Confidence score from user input (0-100)\n",
    "        C_norm = (C - 0) / (100 - 0)  # Normalization (C_min=0, C_max=100)\n",
    "\n",
    "        # Attack Criticality (A)\n",
    "        A = self.attack_criticality.get(attack_type, 5)  # Default to 5 if attack type not found\n",
    "        A_norm = (A - 1) / (10 - 1)  # Normalization (A_min=1, A_max=10)\n",
    "\n",
    "        # Segment Criticality (S)\n",
    "        segment = self.entity_segments.get(entity_id, 'default_segment')\n",
    "        S = self.segment_criticality.get(segment, 5)  # Default to 5\n",
    "        S_norm = (S - 1) / (10 - 1)  # Normalization (S_min=1, S_max=10)\n",
    "\n",
    "        # Past Anomalies (P)\n",
    "        P = self.entity_risks[entity_id]['past_anomalies_count']\n",
    "        P_log = math.log(P + 1)\n",
    "\n",
    "        # Collect P_log values for all entities\n",
    "        P_log_values = [math.log(self.entity_risks[e]['past_anomalies_count'] + 1) for e in self.entity_risks]\n",
    "\n",
    "        # Compute Median and IQR of P_log_values\n",
    "        median_P_log = statistics.median(P_log_values)\n",
    "        Q1 = np.percentile(P_log_values, 25)\n",
    "        Q3 = np.percentile(P_log_values, 75)\n",
    "        IQR_P_log = Q3 - Q1\n",
    "\n",
    "        # Handle division by zero\n",
    "        if IQR_P_log == 0:\n",
    "            P_norm = 0\n",
    "        else:\n",
    "            P_norm = (P_log - median_P_log) / IQR_P_log\n",
    "\n",
    "        # Weights\n",
    "        w_C = 0.25\n",
    "        w_A = 0.35\n",
    "        w_S = 0.20\n",
    "        w_P = 0.20\n",
    "\n",
    "        # Compute Threat Risk\n",
    "        Threat_Risk = (w_C * C_norm) + (w_A * A_norm) + (w_S * S_norm) + (w_P * P_norm)\n",
    "\n",
    "        # Determine Risk Level\n",
    "        risk_level = self.determine_risk_level(Threat_Risk)\n",
    "\n",
    "        # Update risk level\n",
    "        previous_risk_level = self.entity_risks[entity_id]['risk_level']\n",
    "        self.entity_risks[entity_id]['risk_level'] = risk_level\n",
    "\n",
    "        # Get current timestamp\n",
    "        timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "        # Add anomaly to history\n",
    "        self.entity_risks[entity_id]['history'].append({\n",
    "            'timestamp': timestamp,\n",
    "            'attack_type': attack_type,\n",
    "            'confidence': confidence,\n",
    "            'C_norm': C_norm,\n",
    "            'A_norm': A_norm,\n",
    "            'S_norm': S_norm,\n",
    "            'P_norm': P_norm,\n",
    "            'Threat_Risk': Threat_Risk,\n",
    "            'risk_level': risk_level\n",
    "        })\n",
    "\n",
    "        print(f\"Entity '{entity_id}' risk level updated from '{previous_risk_level}' to '{risk_level}'\")\n",
    "\n",
    "        # Save data\n",
    "        self.save_data()\n",
    "\n",
    "        # Notify PE if risk level is 'high risk' or 'critical'\n",
    "        if self.pe_url and risk_level in ['high risk', 'critical']:\n",
    "            self.notify_pe_risk_update(entity_id, risk_level, request_id)\n",
    "\n",
    "        # End CPU and timing measurements\n",
    "        end_time = time.time()\n",
    "        cpu_times_end = process.cpu_times()\n",
    "\n",
    "        # Calculate CPU time and processing time\n",
    "        user_cpu_time = cpu_times_end.user - cpu_times_start.user\n",
    "        system_cpu_time = cpu_times_end.system - cpu_times_start.system\n",
    "        total_cpu_time = user_cpu_time + system_cpu_time\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Log the performance data\n",
    "        logging.info(\n",
    "            f\"Request ID: {request_id} | Entity ID: {entity_id} | Risk Level Updated: {risk_level} | \"\n",
    "            f\"Processing Time: {processing_time:.6f}s | CPU Time: User={user_cpu_time:.6f}s \"\n",
    "            f\"System={system_cpu_time:.6f}s Total={total_cpu_time:.6f}s\"\n",
    "        )\n",
    "\n",
    "        return risk_level\n",
    "\n",
    "    def update_no_anomaly(self, request_id):\n",
    "        # Start CPU and timing measurements\n",
    "        process = psutil.Process(os.getpid())\n",
    "        cpu_times_start = process.cpu_times()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # For each entity, update no_anomaly_rounds and adjust risk levels if necessary\n",
    "        for entity_id, data in self.entity_risks.items():\n",
    "            if data['risk_level'] != 'normal':\n",
    "                data['no_anomaly_rounds'] += 1\n",
    "                risk_level = data['risk_level']\n",
    "                no_anomaly_rounds = data['no_anomaly_rounds']\n",
    "\n",
    "                # Define thresholds for downgrading risk levels\n",
    "                if risk_level == 'low risk' and no_anomaly_rounds >= 1:\n",
    "                    data['risk_level'] = 'normal'\n",
    "                    data['no_anomaly_rounds'] = 0\n",
    "                    print(f\"Entity '{entity_id}' risk level decreased to 'normal'\")\n",
    "                elif risk_level == 'high risk' and no_anomaly_rounds >= 2:\n",
    "                    data['risk_level'] = 'normal'\n",
    "                    data['no_anomaly_rounds'] = 0\n",
    "                    print(f\"Entity '{entity_id}' risk level decreased to 'normal'\")\n",
    "                elif risk_level == 'critical' and no_anomaly_rounds >= 3:\n",
    "                    data['risk_level'] = 'normal'\n",
    "                    data['no_anomaly_rounds'] = 0\n",
    "                    print(f\"Entity '{entity_id}' risk level decreased to 'normal'\")\n",
    "                else:\n",
    "                    print(f\"Entity '{entity_id}' risk level remains at '{risk_level}', no_anomaly_rounds: {no_anomaly_rounds}\")\n",
    "            else:\n",
    "                # Risk level is 'normal'; no action needed\n",
    "                pass\n",
    "\n",
    "        # Save data after updates\n",
    "        self.save_data()\n",
    "\n",
    "        # End CPU and timing measurements\n",
    "        end_time = time.time()\n",
    "        cpu_times_end = process.cpu_times()\n",
    "\n",
    "        # Calculate CPU time and processing time\n",
    "        user_cpu_time = cpu_times_end.user - cpu_times_start.user\n",
    "        system_cpu_time = cpu_times_end.system - cpu_times_start.system\n",
    "        total_cpu_time = user_cpu_time + system_cpu_time\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Log the performance data\n",
    "        logging.info(\n",
    "            f\"Request ID: {request_id} | Action: Update No Anomaly | \"\n",
    "            f\"Processing Time: {processing_time:.6f}s | CPU Time: User={user_cpu_time:.6f}s \"\n",
    "            f\"System={system_cpu_time:.6f}s Total={total_cpu_time:.6f}s\"\n",
    "        )\n",
    "\n",
    "    def determine_risk_level(self, Threat_Risk):\n",
    "        if Threat_Risk >= 0.8:\n",
    "            return 'critical'\n",
    "        elif Threat_Risk >= 0.6:\n",
    "            return 'high risk'\n",
    "        elif Threat_Risk >= 0.4:\n",
    "            return 'low risk'\n",
    "        else:\n",
    "            return 'normal'\n",
    "\n",
    "    def get_risk_status(self, entity_id, request_id):\n",
    "        # Start CPU and timing measurements\n",
    "        process = psutil.Process(os.getpid())\n",
    "        cpu_times_start = process.cpu_times()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Returns the current risk level of the entity\n",
    "        risk_level = self.entity_risks.get(entity_id, {'risk_level': 'normal'})['risk_level']\n",
    "\n",
    "        # End CPU and timing measurements\n",
    "        end_time = time.time()\n",
    "        cpu_times_end = process.cpu_times()\n",
    "\n",
    "        # Calculate CPU time and processing time\n",
    "        user_cpu_time = cpu_times_end.user - cpu_times_start.user\n",
    "        system_cpu_time = cpu_times_end.system - cpu_times_start.system\n",
    "        total_cpu_time = user_cpu_time + system_cpu_time\n",
    "        processing_time = end_time - start_time\n",
    "\n",
    "        # Log the performance data\n",
    "        logging.info(\n",
    "            f\"Request ID: {request_id} | Entity ID: {entity_id} | Action: Get Risk Status | \"\n",
    "            f\"Risk Level: {risk_level} | Processing Time: {processing_time:.6f}s | \"\n",
    "            f\"CPU Time: User={user_cpu_time:.6f}s System={system_cpu_time:.6f}s \"\n",
    "            f\"Total={total_cpu_time:.6f}s\"\n",
    "        )\n",
    "\n",
    "        return risk_level\n",
    "\n",
    "    def get_entity_history(self, entity_id, request_id):\n",
    "        # Returns the history of anomalies for the entity\n",
    "        history = self.entity_risks.get(entity_id, {}).get('history', [])\n",
    "        return history\n",
    "\n",
    "    def notify_pe_risk_update(self, entity_id, risk_level, request_id):\n",
    "        # Notify the PE of the risk update\n",
    "        try:\n",
    "            payload = {\n",
    "                'entity_id': entity_id,\n",
    "                'risk_level': risk_level,\n",
    "                'request_id': request_id\n",
    "            }\n",
    "            response = requests.post(f'{self.pe_url}/receive_risk_update', json=payload)\n",
    "            response.raise_for_status()\n",
    "            logging.info(f\"Request ID: {request_id} | RiskAssessment: Notified PE of risk update for entity '{entity_id}' to '{risk_level}'.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Request ID: {request_id} | RiskAssessment Error: Failed to notify PE of risk update: {e}\")\n",
    "\n",
    "# Flask API Endpoints\n",
    "\n",
    "RiskAssessment_instance = None  # Will be initialized later\n",
    "\n",
    "@app.route('/record_anomaly', methods=['POST'])\n",
    "def record_anomaly_endpoint():\n",
    "    \"\"\"\n",
    "    API endpoint to record an anomaly.\n",
    "    Expects JSON data with 'entity_id', 'attack_type', 'confidence', and optional 'request_id'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = request.get_json()\n",
    "        entity_id = json_data.get('entity_id')\n",
    "        attack_type = json_data.get('attack_type')\n",
    "        confidence = float(json_data.get('confidence', 0))\n",
    "        request_id = json_data.get('request_id', str(uuid.uuid4()))\n",
    "\n",
    "        risk_level = RiskAssessment_instance.record_anomaly(entity_id, attack_type, confidence, request_id)\n",
    "\n",
    "        response = {\n",
    "            'request_id': request_id,\n",
    "            'entity_id': entity_id,\n",
    "            'risk_level': risk_level\n",
    "        }\n",
    "        return jsonify(response), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in record_anomaly_endpoint: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/update_no_anomaly', methods=['POST'])\n",
    "def update_no_anomaly_endpoint():\n",
    "    \"\"\"\n",
    "    API endpoint to update the system when no anomaly is detected.\n",
    "    Expects optional 'request_id'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = request.get_json()\n",
    "        request_id = json_data.get('request_id', str(uuid.uuid4()))\n",
    "\n",
    "        RiskAssessment_instance.update_no_anomaly(request_id)\n",
    "\n",
    "        response = {\n",
    "            'request_id': request_id,\n",
    "            'status': 'No anomaly update processed successfully'\n",
    "        }\n",
    "        return jsonify(response), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in update_no_anomaly_endpoint: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/get_risk_status', methods=['POST'])\n",
    "def get_risk_status_endpoint():\n",
    "    \"\"\"\n",
    "    API endpoint to get the risk status of an entity.\n",
    "    Expects JSON data with 'entity_id' and optional 'request_id'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = request.get_json()\n",
    "        entity_id = json_data.get('entity_id')\n",
    "        request_id = json_data.get('request_id', str(uuid.uuid4()))\n",
    "\n",
    "        risk_level = RiskAssessment_instance.get_risk_status(entity_id, request_id)\n",
    "\n",
    "        response = {\n",
    "            'request_id': request_id,\n",
    "            'entity_id': entity_id,\n",
    "            'risk_level': risk_level\n",
    "        }\n",
    "        return jsonify(response), 200\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in get_risk_status_endpoint: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Set the URL of the Policy Engine\n",
    "    pe_url = 'http://192.52.33.4:5000'  # Replace with actual IP\n",
    "\n",
    "    # Create an instance of RiskAssessment\n",
    "    RiskAssessment_instance = RiskAssessment(pe_url=pe_url)\n",
    "\n",
    "    # Run the Flask app on all interfaces, port 5000\n",
    "    app.run(host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AnomalyTest",
   "language": "python",
   "name": "anomalytest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
